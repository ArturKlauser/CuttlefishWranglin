# Getting started with Docker

Docker is a way to isolate the userspace parts of an OS to a particular process. What this means in practice is if you have a software package that has annoying and/or crazy build system requirements, you can isolate the badness to that particular package and not taint the rest of the system. Such packages are common in bioinformatics, so we're setting up Docker builds for the tools we need so people don't have to spew stuff all over their main OS installs to use them.

For each of these directories with a dockerfile in it, you can build it like so:

```
docker build -f abyss/Dockerfile .
```

That will output a bunch of stuff, ending in something like this:

```
Successfully built 06b350ac51ae
```

That bit of hex at the end is the *image id* that contains the files you care about (e.g. compiled software).

> If you are comfortable with docker tags, docker hub, etc you can use tags and push/pull your own images, etc, but for now we will not assume that knowledge, and we'll do everything with local builds and image ids.

## Running a temporary container

To run a temporary container that throws away any changes made once the container exits, you would run:

```
docker run -i --rm -t <image id> bash
```

The options `-i` and `-t` set up the container for interactive shell usage, and `--rm`: remove the image generated by whatever work you do inside the shell. If you didn't have `--rm` your disk will gradually fill up with all the (useless) images generated by your interactive work.

That command will give you a root shell (`bash`, specifically) inside the image. You can poke around (`ls`, `cd`, etc) to satisfy yourself that the files you expect to have are there. If they aren't, maybe the Dockerfile isn't working, or isn't documented clearly, so file a bug or make a PR.

## Running commands from the host OS

Just an interactive shell with the contents of the image usually isn't what you want. You probably want to run an actual command that's part of the software built by the Dockerfile. Fortunately, this is easy. Instead of `bash` in the example above, use the path *inside the image* of the command you want. In the case of `sra-tools`, for instance, it installs stuff in weird locations (remember, this is why we don't want this installed in our host OS), but we can still invoke it from the host OS:

```
docker run -i --rm -t <image id> /usr/local/ncbi/sra-tools/bin/fastq-dump --help
```

That should show the help for `fastq-dump`, and of course you could run any other command the same way.

# Accessing files inside containers

Dockerfiles have a way to include files in the resulting images, but it requires rebuilding the images every time you change a file, so it's not really suited to the sort of data manipulation we want to do. It's oriented more towards including source code or config, not large data files. What we want to do is use a *volume*, which is a way to have a directory in the host OS that's also visible inside the container without copying the data. The same data is accessible in both places.

First, create a directory in your host system. If you have a place you want to keep your large files (a big hard drive or equivalent, `/big-volume` below), put the directory there. 

```
mkdir /big-volume/bio-data
```

Then, run a command that downloads some data 

```
docker run -i --rm -t <image id> -v /big-volume/bio-data:/data /usr/local/ncbi/sra-tools/bin/fastq-dump --outdir /data --gzip --split-files ERR1294016
```

That will take a long time to run. While it's running, you can use a different shell to check the progress of the command by watching the files grow in `/big-volume/bio-data`.

When the command completes, the temporary container will go away, but the data will remain for you to use with subsequent commands.
